{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83e\udd16 Chapter 13: 3D Deep Learning with GeoSAM\n",
                "\n",
                "Traditional segmentation (clustering) relies on geometry. Deep Learning models like **SAM (Segment Anything Model)** allow us to segment based on semantic features in images, and then project those segmentations to 3D.\n",
                "\n",
                "**Workflow:**\n",
                "1.  **Project** 3D point cloud to a 2D spherical image (panorama).\n",
                "2.  **Segment** the image using SAM (Meta's Foundation Model).\n",
                "3.  **Back-project** the 2D masks to 3D points.\n",
                "\n",
                "**Note:** This chapter requires `torch` and `segment_anything` libraries, and the SAM checkpoint weights."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import laspy\n",
                "import torch\n",
                "import time"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "\n",
                "We load a LiDAR scan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "filename = \"../DATA/ITC_BUILDING.las\"\n",
                "\n",
                "try:\n",
                "    las = laspy.read(filename)\n",
                "    coords = np.vstack((las.x, las.y, las.z)).transpose()\n",
                "    # Extract color if available, else standard grey\n",
                "    try:\n",
                "        r = (las.red / 65535 * 255).astype(int)\n",
                "        g = (las.green / 65535 * 255).astype(int)\n",
                "        b = (las.blue / 65535 * 255).astype(int)\n",
                "        colors = np.vstack((r, g, b)).transpose()\n",
                "    except:\n",
                "        colors = np.ones_like(coords) * 128\n",
                "        \n",
                "    print(f\"Loaded {len(coords)} points.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading data: {e}\")\n",
                "    # Dummy data for demonstration\n",
                "    coords = np.random.rand(1000, 3) * 100\n",
                "    colors = np.random.randint(0, 255, (1000, 3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Generate Spherical Image (3D to 2D)\n",
                "\n",
                "We obtain a 360\u00b0 panoramic view from the center of the point cloud."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_spherical_image(center_coordinates, point_cloud, colors, resolution_y=500):\n",
                "    translated_points = point_cloud - center_coordinates\n",
                "\n",
                "    # Spherical coordinates\n",
                "    theta = np.arctan2(translated_points[:, 1], translated_points[:, 0])\n",
                "    phi = np.arccos(translated_points[:, 2] / (np.linalg.norm(translated_points, axis=1) + 1e-6))\n",
                "\n",
                "    # Map to pixels\n",
                "    x = (theta + np.pi) / (2 * np.pi) * (2 * resolution_y)\n",
                "    y = phi / np.pi * resolution_y\n",
                "\n",
                "    resolution_x = 2 * resolution_y\n",
                "    image = np.zeros((resolution_y, resolution_x, 3), dtype=np.uint8)\n",
                "    mapping = np.full((resolution_y, resolution_x), -1, dtype=int)\n",
                "\n",
                "    # Z-buffer check (keep closest point)\n",
                "    dists = np.linalg.norm(translated_points, axis=1)\n",
                "    \n",
                "    # Vectorized or simple loop (loop is slow but clear for demo)\n",
                "    # Ideally vectorized, but here we stick to the provided logic for clarity\n",
                "    for i in range(len(translated_points)):\n",
                "        ix = np.clip(int(x[i]), 0, resolution_x - 1)\n",
                "        iy = np.clip(int(y[i]), 0, resolution_y - 1)\n",
                "        \n",
                "        existing_idx = mapping[iy, ix]\n",
                "        if existing_idx == -1 or dists[i] < dists[existing_idx]:\n",
                "            mapping[iy, ix] = i\n",
                "            image[iy, ix] = colors[i]\n",
                "\n",
                "    return image, mapping\n",
                "\n",
                "# Center of the building (approximate)\n",
                "center = np.mean(coords, axis=0)\n",
                "start = time.time()\n",
                "spherical_img, pixel_to_point_map = generate_spherical_image(center, coords, colors, resolution_y=400)\n",
                "print(f\"Projection took {time.time() - start:.2f}s\")\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.imshow(spherical_img)\n",
                "plt.title(\"Spherical Projection\")\n",
                "plt.axis('off')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Segment with SAM\n",
                "\n",
                "We feed this 2D image to SAM. SAM returns masks (segmentations) for \"everything\" it sees."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pseudo-code if SAM is not installed or weights missing\n",
                "try:\n",
                "    from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
                "    \n",
                "    # You need to download the checkpoint manually\n",
                "    CHECKPOINT_PATH = \"../../MODELS/sam_vit_h_4b8939.pth\"\n",
                "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "    \n",
                "    print(f\"Loading SAM on {DEVICE}...\")\n",
                "    sam = sam_model_registry[\"vit_h\"](checkpoint=CHECKPOINT_PATH)\n",
                "    sam.to(device=DEVICE)\n",
                "    \n",
                "    mask_generator = SamAutomaticMaskGenerator(sam)\n",
                "    masks = mask_generator.generate(spherical_img)\n",
                "    \n",
                "    print(f\"Generated {len(masks)} masks.\")\n",
                "    \n",
                "    # Visualize masks on image\n",
                "    # (Visualization code would go here)\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"scale_anything library not installed.\")\n",
                "except Exception as e:\n",
                "    print(f\"Could not run SAM: {e}\")\n",
                "    masks = [] # Empty result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Back-Project to 3D\n",
                "\n",
                "We use the `pixel_to_point_map` to assign segment IDs back to the original 3D points."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if len(masks) > 0:\n",
                "    # Assign a random color to each point based on its mask\n",
                "    segmented_colors = np.copy(colors)\n",
                "    \n",
                "    for mask_data in masks:\n",
                "        segmentation = mask_data['segmentation'] # boolean 2D array\n",
                "        \n",
                "        # Random color for this segment\n",
                "        rnd_color = np.random.randint(0, 255, 3)\n",
                "        \n",
                "        # Get pixels belonging to this mask\n",
                "        ys, xs = np.where(segmentation)\n",
                "        \n",
                "        # Find corresponding 3D points\n",
                "        point_indices = pixel_to_point_map[ys, xs]\n",
                "        valid_indices = point_indices[point_indices != -1]\n",
                "        \n",
                "        # Colorize\n",
                "        segmented_colors[valid_indices] = rnd_color\n",
                "        \n",
                "    # Save/Visualize\n",
                "    # To visualize in Open3D, we'd create a new PCD\n",
                "    import open3d as o3d\n",
                "    pcd_seg = o3d.geometry.PointCloud()\n",
                "    pcd_seg.points = o3d.utility.Vector3dVector(coords)\n",
                "    pcd_seg.colors = o3d.utility.Vector3dVector(segmented_colors / 255.0)\n",
                "    o3d.visualization.draw_geometries([pcd_seg], window_name=\"GeoSAM Result\")\n",
                "    \n",
                "else:\n",
                "    print(\"No masks generated (SAM skipped).\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}