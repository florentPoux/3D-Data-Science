{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "General Information\n",
    "* Created by: \ud83e\udd8a Florent Poux. \n",
    "* Copyright: Florent Poux.\n",
    "* License: MIT\n",
    "* Status: Confidential\n",
    "\n",
    "Dependencies:\n",
    "* Anaconda or Miniconda\n",
    "* An Anaconda new environment\n",
    "* Libraries as described in the Chapter\n",
    "\n",
    "Have fun with this Code Solution.\n",
    "\n",
    "\ud83c\udfb5 Note: Styling was not taken care of at this stage.\n",
    "\n",
    "Enjoy!\n",
    "\"\"\"\n",
    "\n",
    "#%% Libraries import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import laspy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#%%\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "class PointCloudCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super(PointCloudCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(input_channels, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool3d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool3d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool3d(x, kernel_size=2, stride=2)\n",
    "        \n",
    "        x = F.adaptive_avg_pool3d(x, (1, 1, 1))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "num_classes = 10  # Adjust based on your dataset\n",
    "model = PointCloudCNN(num_classes)\n",
    "\n",
    "# Example input (batch_size, channels, depth, height, width)\n",
    "example_input = torch.randn(1, 1, 64, 64, 64)\n",
    "output = model(example_input)\n",
    "print(output.shape)  # Should be (1, num_classes)\n",
    "    \n",
    "#%% Definition of training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=2)\n",
    "        correct += (pred == target).sum().item()\n",
    "        total += target.numel()\n",
    "    return total_loss / len(train_loader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=2)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.numel()\n",
    "    return total_loss / len(val_loader), correct / total\n",
    "\n",
    "#%% Definition of Class\n",
    "\n",
    "class LasPointCloudDataset(Dataset):\n",
    "    def __init__(self, las_file_path, num_points=4096, voxel_size=1.0, features=['intensity', 'return_number', 'number_of_returns']):\n",
    "        self.num_points = num_points\n",
    "        self.voxel_size = voxel_size\n",
    "        self.features = features\n",
    "\n",
    "        # Read the .las file\n",
    "        las = laspy.read(las_file_path)\n",
    "\n",
    "        # Extract point cloud data\n",
    "        points = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "        \n",
    "        # Extract features\n",
    "        feature_data = []\n",
    "        for feature in features:\n",
    "            if hasattr(las, feature):\n",
    "                feature_data.append(getattr(las, feature))\n",
    "            else:\n",
    "                print(f\"Warning: Feature '{feature}' not found in the .las file.\")\n",
    "        \n",
    "        feature_data = np.vstack(feature_data).transpose() if feature_data else np.zeros((len(points), 0))\n",
    "        \n",
    "        # Combine points and features\n",
    "        self.point_cloud = np.hstack((points, feature_data))\n",
    "        self.labels = las.classification\n",
    "\n",
    "        # Normalize spatial coordinates\n",
    "        self.scaler = StandardScaler()\n",
    "        self.point_cloud[:, :3] = self.scaler.fit_transform(self.point_cloud[:, :3])\n",
    "\n",
    "        # Create a KD-tree for efficient nearest neighbor search\n",
    "        self.kdtree = cKDTree(self.point_cloud[:, :3])\n",
    "        \n",
    "        # Get unique classes\n",
    "        self.unique_classes = np.unique(self.labels)\n",
    "        self.num_classes = len(self.unique_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.point_cloud) // self.num_points\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Randomly select a center point\n",
    "        center_idx = np.random.randint(0, len(self.point_cloud))\n",
    "        center = self.point_cloud[center_idx]\n",
    "\n",
    "        # Find nearest neighbors\n",
    "        _, indices = self.kdtree.query(center, k=self.num_points)\n",
    "        sample = self.point_cloud[indices]\n",
    "        sample_labels = self.labels[indices]\n",
    "\n",
    "        # Voxelize the sample\n",
    "        voxelized, voxel_labels = self.voxelize(sample, sample_labels)\n",
    "\n",
    "        # Convert to tensor\n",
    "        voxelized = torch.FloatTensor(voxelized)\n",
    "        voxel_labels = torch.LongTensor(voxel_labels)\n",
    "\n",
    "        return voxelized, voxel_labels\n",
    "\n",
    "    def voxelize(self, points, labels):\n",
    "        # Determine the number of voxels in each dimension\n",
    "        voxel_dims = np.ceil((points.max(axis=0) - points.min(axis=0)) / self.voxel_size).astype(int)\n",
    "        \n",
    "        # Initialize voxel grid and label grid\n",
    "        voxel_grid = np.zeros((1, *voxel_dims))  # Only one channel for point count\n",
    "        label_grid = np.zeros(voxel_dims, dtype=int)\n",
    "\n",
    "        # Voxelize points and labels\n",
    "        for point, label in zip(points, labels):\n",
    "            voxel_index = tuple((point - points.min(axis=0)) // self.voxel_size)\n",
    "            voxel_grid[0][voxel_index] += 1  # Count of points in voxel\n",
    "            label_grid[voxel_index] = label  # Assign label to voxel (last label wins)\n",
    "\n",
    "        return voxel_grid, label_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(las_file_path, batch_size=32, num_points=4096, voxel_size=1.0, features=['intensity', 'return_number', 'number_of_returns']):\n",
    "    dataset = LasPointCloudDataset(las_file_path, num_points, voxel_size, features)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "#%% Execution\n",
    "\n",
    "# Assuming you have a dataset of point clouds and labels\n",
    "# Each point cloud is a numpy array of shape (num_points, 3)\n",
    "# Labels are numpy arrays of shape (num_points,)\n",
    "\n",
    "# Set up the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the features to use\n",
    "features = []\n",
    "# features = ['intensity', 'return_number', 'number_of_returns']\n",
    "\n",
    "# Load and split the data\n",
    "las_file_path = 'DATA/indoor_test.las'\n",
    "full_dataset = LasPointCloudDataset(las_file_path, num_points=4096, voxel_size=1.0, features=features)\n",
    "\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, num_workers=4)\n",
    "\n",
    "# Initialize the model\n",
    "num_classes = full_dataset.num_classes\n",
    "input_channels = len(features) + 1  # +1 for the point count in each voxel\n",
    "model = PointCloudCNN(num_classes, input_channels).to(device)\n",
    "\n",
    "# Choose loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_loss, test_accuracy = validate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "#%% Prediction and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, test_loader, original_las_path, output_las_path, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions = output.argmax(dim=2).cpu().numpy()\n",
    "            all_predictions.extend(predictions.reshape(-1))\n",
    "\n",
    "    # Read the original .las file\n",
    "    with laspy.open(original_las_path) as original_las:\n",
    "        # Create a new .las file with the same properties as the original\n",
    "        with laspy.open(output_las_path, mode=\"w\", header=original_las.header) as output_las:\n",
    "            # Copy all points from the original file to the new file\n",
    "            output_las.write_points(original_las.read_points())\n",
    "\n",
    "            # Add a new field for predictions\n",
    "            output_las.add_extra_dim(laspy.ExtraBytesParams(name=\"prediction\", type=np.int32))\n",
    "\n",
    "            # Write predictions to the new field\n",
    "            output_las.prediction = np.array(all_predictions[:len(output_las.points)])\n",
    "\n",
    "    print(f\"Predictions saved to {output_las_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}